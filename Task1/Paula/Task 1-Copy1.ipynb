{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c13b595b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9900990099009901, 0.896551724137931, 1.009090909090909, 0.9564814814814815], [0.9900990099009901, 1.0, 1.0, 0.9938271604938272], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936, 0.9783950617283951], [1.0, 0.896551724137931, 1.009090909090909, 0.954320987654321], [0.9900990099009901, 1.0, 1.0, 1.0], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936, 0.9691358024691358], [1.188118811881188, 0.896551724137931, 1.009090909090909, 1.096604938271605], [1.7821782178217822, 1.0, 1.0, 1.4320987654320987]]\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.23 0.4 0.7 0.72 0.7\n",
      "-0.15 -0.12 0.01\n",
      "E3 =  0.0022821697867831175 icount = 8\n",
      "next ws: 1.2296083538584661 0.3995281787508194 0.6995380344292121 0.719764132121992 0.6997610934799811\n",
      "next bs: -0.15037876716233242 -0.12035397558005352 0.009678257751903757\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2296083538584661 0.3995281787508194 0.6995380344292121 0.719764132121992 0.6997610934799811\n",
      "-0.15037876716233242 -0.12035397558005352 0.009678257751903757\n",
      "E3 =  0.002089839976739339 icount = 8\n",
      "next ws: 1.2292314331859875 0.39907326173345536 0.6990924992586383 0.7195384968149412 0.6995325035360976\n",
      "next bs: -0.15074294717948497 -0.1206939829129742 0.009369616233548147\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2292314331859875 0.39907326173345536 0.6990924992586383 0.7195384968149412 0.6995325035360976\n",
      "-0.15074294717948497 -0.1206939829129742 0.009369616233548147\n",
      "E3 =  0.0019144317431043821 icount = 8\n",
      "next ws: 1.2288684203311615 0.3986342744996183 0.698662441487544 0.7193225635399163 0.6993136962078762\n",
      "next bs: -0.15109333738810507 -0.12102077661908257 0.009073375711823765\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2288684203311615 0.3986342744996183 0.698662441487544 0.7193225635399163 0.6993136962078762\n",
      "-0.15109333738810507 -0.12102077661908257 0.009073375711823765\n",
      "E3 =  0.0017544339852196933 icount = 8\n",
      "next ws: 1.2285185209244123 0.39821026627737377 0.6982469306238016 0.7191158214283196 0.6991041571072367\n",
      "next bs: -0.15143071095155952 -0.12133508703337753 0.008788860550977015\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2285185209244123 0.39821026627737377 0.6982469306238016 0.7191158214283196 0.6991041571072367\n",
      "-0.15143071095155952 -0.12133508703337753 0.008788860550977015\n",
      "E3 =  0.0016084725338212289 icount = 8\n",
      "next ws: 1.2281809610632184 0.39780030609795 0.6978450548109004 0.7189177780190342 0.6989033901556106\n",
      "next bs: -0.1517558194264228 -0.1216376225047974 0.008515417204452626\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2281809610632184 0.39780030609795 0.6978450548109004 0.7189177780190342 0.6989033901556106\n",
      "-0.1517558194264228 -0.1216376225047974 0.008515417204452626\n",
      "E3 =  0.0014752977415484218 icount = 8\n",
      "next ws: 1.2278549842442894 0.397403478489191 0.6974559165090691 0.718727957955023 0.698710916276546\n",
      "next bs: -0.1520693955319336 -0.12192907184996453 0.008252412102521681\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2278549842442894 0.397403478489191 0.6974559165090691 0.718727957955023 0.698710916276546\n",
      "-0.1520693955319336 -0.12192907184996453 0.008252412102521681\n",
      "E3 =  0.001353773214625864 icount = 8\n",
      "next ws: 1.2275398479621278 0.39701887860148316 0.6970786275927433 0.7185459016211981 0.6985262720251468\n",
      "next bs: -0.15237215618872132 -0.1222101070145694 0.00799922939577676\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2275398479621278 0.39701887860148316 0.6970786275927433 0.7185459016211981 0.6985262720251468\n",
      "-0.15237215618872132 -0.1222101070145694 0.00799922939577676\n",
      "E3 =  0.0012428655838101095 icount = 8\n",
      "next ws: 1.2272348198679832 0.39664560658801584 0.6967123036812144 0.7183711637001942 0.6983490081303093\n",
      "next bs: -0.1526648059136566 -0.12248138601167056 0.007755268502857537\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2272348198679832 0.39664560658801584 0.6967123036812144 0.7183711637001942 0.6983490081303093\n",
      "-0.1526648059136566 -0.12248138601167056 0.007755268502857537\n",
      "E3 =  0.0011416352227580908 icount = 8\n",
      "next ws: 1.2269391733479666 0.3962827609997456 0.6963560574557159 0.7182033116156639 0.6981786879184346\n",
      "next bs: -0.1529480406860322 -0.12274355622834546 0.00751994139472594\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2269391733479666 0.3962827609997456 0.6963560574557159 0.7182033116156639 0.6981786879184346\n",
      "-0.1529480406860322 -0.12274355622834546 0.00751994139472594\n",
      "E3 =  0.001049227831403467 icount = 8\n",
      "next ws: 1.2266521823295518 0.3959294308680943 0.6960089906257529 0.7180419238230862 0.6980148855772963\n",
      "next bs: -0.1532225524399669 -0.12299725822302258 0.0072926695255365135\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2266521823295518 0.3959294308680943 0.6960089906257529 0.7180419238230862 0.6980148855772963\n",
      "-0.1532225524399669 -0.12299725822302258 0.0072926695255365135\n",
      "E3 =  0.0009648668109091062 icount = 8\n",
      "next ws: 1.2263731150548962 0.3955846860222555 0.6956701840766083 0.7178865878946477 0.6978571842047999\n",
      "next bs: -0.15348903439435713 -0.12324313017957288 0.007072880288758875\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2263731150548962 0.3955846860222555 0.6956701840766083 0.7178865878946477 0.6978571842047999\n",
      "-0.15348903439435713 -0.12324313017957288 0.007072880288758875\n",
      "E3 =  0.0008878463655383753 icount = 8\n",
      "next ws: 1.2261012264564692 0.3952475650023508 0.6953386855370333 0.7177368983257455 0.6977051735676221\n",
      "next bs: -0.1537481875133356 -0.12348181324723284 0.0068600028322442315\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2261012264564692 0.3952475650023508 0.6953386855370333 0.7177368983257455 0.6977051735676221\n",
      "-0.1537481875133356 -0.12348181324723284 0.0068600028322442315\n",
      "E3 =  0.0008175252756878874 icount = 8\n",
      "next ws: 1.2258357486178455 0.39491705965089724 0.6950134938158022 0.7175924539632458 0.6975584474662394\n",
      "next bs: -0.15400072851054628 -0.12371395808783837 0.0066534630003438995\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2258357486178455 0.39491705965089724 0.6950134938158022 0.7175924539632458 0.6975584474662394\n",
      "-0.15400072851054628 -0.12371395808783837 0.0066534630003438995\n",
      "E3 =  0.0007533212958426539 icount = 8\n",
      "next ws: 1.2255758785713236 0.39459209503673837 0.6946935382087696 0.717452854915346 0.6974166005609932\n",
      "next bs: -0.15424739999172182 -0.12394023309020089 0.006452677073602814\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2255758785713236 0.39459209503673837 0.6946935382087696 0.717452854915346 0.6974166005609932\n",
      "-0.15424739999172182 -0.12394023309020089 0.006452677073602814\n",
      "E3 =  0.0006947061421201803 icount = 8\n",
      "next ws: 1.2253207623253062 0.3942715026905964 0.6943776509717919 0.7173176987424179 0.6972792244509667\n",
      "next bs: -0.15448898360924943 -0.12416133492338458 0.00625704382808915\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2253207623253062 0.3942715026905964 0.6943776509717919 0.7173176987424179 0.6972792244509667\n",
      "-0.15448898360924943 -0.12416133492338458 0.00625704382808915\n",
      "E3 =  0.0006412010476434136 icount = 8\n",
      "next ws: 1.2250694734426089 0.3939539840366296 0.6940645296061952 0.7171865756352338 0.6971459027007533\n",
      "next bs: -0.1547263175440973 -0.12437800243369447 0.0060659342048556735\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2250694734426089 0.3939539840366296 0.6940645296061952 0.7171865756352338 0.6971459027007533\n",
      "-0.1547263175440973 -0.12437800243369447 0.0060659342048556735\n",
      "E3 =  0.0005923728824294785 icount = 8\n",
      "next ws: 1.224820983551188 0.39363805906979 0.6937526837724962 0.7170590621401525 0.6970162043574004\n",
      "next bs: -0.15496032035029284 -0.12459103542897774 0.005878677508517291\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.224820983551188 0.39363805906979 0.6937526837724962 0.7170590621401525 0.6970162043574004\n",
      "-0.15496032035029284 -0.12459103542897774 0.005878677508517291\n",
      "E3 =  0.0005478308618029602 icount = 8\n",
      "next ws: 1.2245741205695506 0.39332199213680663 0.6934403582764574 0.7169347127517662 0.6968896752508814\n",
      "next bs: -0.15519202441073254 -0.12480132079478591 0.005694542438428589\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2245741205695506 0.39332199213680663 0.6934403582764574 0.7169347127517662 0.6968896752508814\n",
      "-0.15519202441073254 -0.12480132079478591 0.005694542438428589\n",
      "E3 =  0.0005072239111217941 icount = 8\n",
      "next ws: 1.2243275075950166 0.3930036808774796 0.6931254174187395 0.7168130482902946 0.6967658259513357\n",
      "next bs: -0.1554226243809493 -0.1250098699520333 0.00551271019776017\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2243275075950166 0.3930036808774796 0.6931254174187395 0.7168130482902946 0.6967658259513357\n",
      "-0.1554226243809493 -0.1250098699520333 0.00551271019776017\n",
      "E3 =  0.0004702388299588592 icount = 8\n",
      "next ws: 1.2240794701371307 0.3926804832847814 0.6928051641720884 0.7166935392725834 0.696644114517666\n",
      "next bs: -0.1556535499030852 -0.12521787450829985 0.005332235026325702\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2240794701371307 0.3926804832847814 0.6928051641720884 0.7166935392725834 0.696644114517666\n",
      "-0.1556535499030852 -0.12521787450829985 0.005332235026325702\n",
      "E3 =  0.0004365995377326625 icount = 8\n",
      "next ws: 1.223827889028329 0.392348935267024 0.6924760434550048 0.7165755811793898 0.6965239208090781\n",
      "next bs: -0.15588657944074197 -0.12542679239656876 0.005151983915249946\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.223827889028329 0.392348935267024 0.6924760434550048 0.7165755811793898 0.6965239208090781\n",
      "-0.15588657944074197 -0.12542679239656876 0.005151983915249946\n",
      "E3 =  0.0004060679572057384 icount = 8\n",
      "next ws: 1.223569954568793 0.392004261607868 0.6921331253679894 0.7164584559699769 0.6964045064660548\n",
      "next bs: -0.15612402774363426 -0.12563848789406057 0.004970540056966451\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.223569954568793 0.392004261607868 0.6921331253679894 0.7164584559699769 0.6964045064660548\n",
      "-0.15612402774363426 -0.12563848789406057 0.004970540056966451\n",
      "E3 =  0.00037844767875945865 icount = 8\n",
      "next ws: 1.2233017276293439 0.391639464105875 0.6917691346203103 0.7163412688447229 0.6962849490732258\n",
      "next bs: -0.15636907456855254 -0.12585547338599584 0.0047860389877187\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2233017276293439 0.391639464105875 0.6917691346203103 0.7163412688447229 0.6962849490732258\n",
      "-0.15636907456855254 -0.12585547338599584 0.0047860389877187\n",
      "E3 =  0.0003535929247621444 icount = 8\n",
      "next ws: 1.2230172864829028 0.39124344715733606 0.6913724363798404 0.7162228369942282 0.6961640261803291\n",
      "next bs: -0.1566263895322491 -0.12608136028595118 0.004595869263480593\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2230172864829028 0.39124344715733606 0.6913724363798404 0.7162228369942282 0.6961640261803291\n",
      "-0.1566263895322491 -0.12608136028595118 0.004595869263480593\n",
      "E3 =  0.0003314289718422072 icount = 8\n",
      "next ws: 1.2227068652491406 0.3907966062292205 0.690922232882917 0.7161014755190219 0.6960399917783959\n",
      "next bs: -0.15690345606793335 -0.12632179142345867 0.004396069536892781\n",
      "\n",
      "E3 =  0.0003314289718422072\n",
      "\n",
      "last w01, w02, w03, w12, w23:\n",
      "last b1, b2, b3:\n",
      "1.2230172864829028 0.39124344715733606 0.6913724363798404 0.7162228369942282 0.6961640261803291\n",
      "-0.1566263895322491 -0.12608136028595118 0.004595869263480593\n",
      "Tdbin, Twbin, qdot, Tdbout, ypredicted:\n",
      "20.0 13.0 310.8 30.97 31.27414918722601\n",
      "20.0 14.5 308.0 32.3 31.82645765372841\n",
      "20.0 15.3 306.0 31.5 32.10264881572437\n",
      "20.2 13.0 310.8 30.91 31.469770412196315\n",
      "20.0 14.5 308.0 32.5 31.82645765372841\n",
      "20.0 15.3 306.0 31.4 32.10264881572437\n",
      "24.0 13.0 310.8 35.59 35.18657368663223\n",
      "36.0 14.5 308.0 46.4 47.476155651353274\n"
     ]
    }
   ],
   "source": [
    "# TASK 1.1\n",
    "\n",
    "'''#Intro to Neural Network Modeling\n",
    "\n",
    "# Python Neural Network Model of Spray Cooling Test System\n",
    ">>>>> start CodeP2.1F23\n",
    "V.P. Carey, ME249, Fall 2023'''\n",
    "# version 3 print function\n",
    "from __future__ import print_function\n",
    "# import math, numpy and other usefuk packages\n",
    "import math\n",
    "import numpy\n",
    "%matplotlib inline\n",
    "# importing the required module\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 8] # for square canvas\n",
    "\n",
    "#assembling data array\n",
    "#store array where rows are data vectors [x01, x02, x03, y3]\n",
    "xydata = []\n",
    "xydata = [[20./20.2, 13.0/14.5, 310.8/308.0, 30.99/32.4], [20./20.2, 14.5/14.5, 308.0/308.0, 32.2/32.4]]\n",
    "xydata.append([20./20.2, 15.3/14.5, 306.0/308.0, 31.7/32.4])\n",
    "xydata.append([20.2/20.2, 13.0/14.5, 310.8/308.0, 30.92/32.4])\n",
    "xydata.append([20./20.2, 14.5/14.5, 308.0/308.0, 32.4/32.4])\n",
    "xydata.append([20./20.2, 15.3/14.5, 306.0/308.0, 31.4/32.4])\n",
    "xydata.append([24./20.2, 13.0/14.5, 310.8/308.0, 35.53/32.4])\n",
    "xydata.append([36./20.2, 14.5/14.5, 308.0/308.0, 46.4/32.4])\n",
    "print (xydata)\n",
    "#set starting values\n",
    "w01n = 1.23\n",
    "w02n = 0.40\n",
    "w03n = 0.70\n",
    "b1n = -0.15\n",
    "w12n = 0.72\n",
    "b2n = -0.12\n",
    "w23n = 0.7\n",
    "b3n = 0.01\n",
    "#start of batch loop\n",
    "for k in range (0,200):\n",
    "    icount = 0\n",
    "    #initialize error and derivative parameters\n",
    "    E3ti = 0.\n",
    "    dE3da3 = 0.\n",
    "    dE3dw01ti = 0.\n",
    "    dE3dw02ti = 0.\n",
    "    dE3dw03ti = 0.\n",
    "    dE3db1ti = 0.\n",
    "    dE3dw12ti = 0.\n",
    "    dE3db2ti = 0.\n",
    "    dE3dw23ti = 0.\n",
    "    dE3db3ti = 0.\n",
    "    w01 = w01n\n",
    "    w02 = w02n\n",
    "    w03 = w03n\n",
    "    b1 = b1n\n",
    "    w12 = w12n\n",
    "    b2 = b2n\n",
    "    w23 = w23n\n",
    "    b3 = b3n\n",
    "    #doing calcuations for each data point\n",
    "    for i in range(0,8):\n",
    "        #compute activation functions and their derivatives\n",
    "        z1 = w01*xydata[i][0]+w02*xydata[i][1]+w03*xydata[i][2]+b1\n",
    "        sig1 = z1\n",
    "        sigp1 = 1.0\n",
    "        if z1 < 0.0:\n",
    "            sig1 = math.exp(z1) - 1.0\n",
    "            sigp1 = math.exp(z1)\n",
    "        a1 = sig1\n",
    "        z2 = w12*a1+b2\n",
    "        sig2 = z2\n",
    "        sigp2 = 1.0\n",
    "        if z2 < 0.0:\n",
    "            sig2 = math.exp(z2) - 1.0\n",
    "            sigp2 = math.exp(z2)\n",
    "        a2 = sig2\n",
    "        z3 = w23*a2+b3\n",
    "        sig3 = z3\n",
    "        sigp3 = 1.0\n",
    "        if z3 < 0.0:\n",
    "            sig3 = math.exp(z3) - 1.0\n",
    "            sigp3 = math.exp(z3)\n",
    "        a3 = sig3\n",
    "\n",
    "        #compute derivatives for backpropagation\n",
    "        #add to sum for batch average calculation\n",
    "        E3ti = E3ti +(a3 - xydata[i][3])*(a3 - xydata[i][3])\n",
    "        dE3da3 = 2.*(a3 - xydata[i][3])\n",
    "        dE3dw01ti = dE3dw01ti + dE3da3*sigp3*w23*sigp2*w12*sigp1*xydata[i][0]\n",
    "        dE3dw02ti = dE3dw02ti + dE3da3*sigp3*w23*sigp2*w12*sigp1*xydata[i][1]\n",
    "        dE3dw03ti = dE3dw03ti + dE3da3*sigp3*w23*sigp2*w12*sigp1*xydata[i][2]\n",
    "        dE3db1ti = dE3dw03ti + dE3da3*sigp3*w23*sigp2*w12*sigp1\n",
    "        dE3dw12ti =dE3dw03ti + dE3da3*sigp3*w23*sigp2*a1\n",
    "        dE3db2ti =dE3dw03ti + dE3da3*sigp3*w23*sigp2\n",
    "        dE3dw23ti =dE3dw03ti + dE3da3*sigp3*a2\n",
    "        dE3db3ti =dE3dw03ti + dE3da3*sigp3\n",
    "        icount = i + 1\n",
    "        # end calculations for each data point in batch\n",
    "    #compute batch averaged values\n",
    "    E3 = E3ti/icount\n",
    "    dE3dw01 = dE3dw01ti/icount\n",
    "    dE3dw02 = dE3dw02ti/icount\n",
    "    dE3dw03 = dE3dw03ti/icount\n",
    "    dE3db1 = dE3db1ti/icount\n",
    "    dE3dw12 = dE3dw12ti/icount\n",
    "    dE3db2 = dE3db2ti/icount\n",
    "    dE3dw23 = dE3dw23ti/icount\n",
    "    dE3db3 = dE3db3ti/icount\n",
    "    #set gam = learning rate\n",
    "    \n",
    "    gam = 0.01              # THIS IS WHAT I HAVE TO ADJUST\n",
    "    \n",
    "    if E3 < 0.07:\n",
    "        gam = 0.009\n",
    "    w01n = w01 + gam*(-E3)/dE3dw01\n",
    "    w02n = w02 + gam*(-E3)/dE3dw02\n",
    "    w03n = w03 + gam*(-E3)/dE3dw03\n",
    "    b1n = b1 + gam*(-E3)/dE3db1\n",
    "    w12n = w12 + gam*(-E3)/dE3dw12\n",
    "    b2n = b2 + gam*(-E3)/dE3db2\n",
    "    w23n = w23 + gam*(-E3)/dE3dw23\n",
    "    b3n = b3 + gam*(-E3)/dE3db3\n",
    "    #printing for each iteration\n",
    "    print ('last w01, w02, w03, w12, w23:')\n",
    "    print ('last b1, b2, b3:')\n",
    "    print (w01, w02, w03, w12, w23)\n",
    "    print (b1, b2, b3)\n",
    "    print ('E3 = ', E3, 'icount =', icount)\n",
    "    print ('next ws:', w01n, w02n, w03n, w12n, w23n)\n",
    "    print ('next bs:', b1n, b2n, b3n)\n",
    "    #quit if squared error is below target\n",
    "    if E3 < 0.00035:\n",
    "        break\n",
    "\n",
    "print()\n",
    "print ('E3 = ', E3)\n",
    "print()\n",
    "\n",
    "print ('last w01, w02, w03, w12, w23:')\n",
    "print ('last b1, b2, b3:')\n",
    "print (w01, w02, w03, w12, w23)\n",
    "print (b1, b2, b3)\n",
    "#decomment print statements below if you want to print neuron outputs\n",
    "#print ('z1 =', z1)\n",
    "#print ('a1 =', a1)\n",
    "#print ('z2 =', z2)\n",
    "#print ('a2 =', a2)\n",
    "#print ('z3 =', z3)\n",
    "#print ('a3 =', a3)\n",
    "#print comparison of data and trained network predictions\n",
    "# restore raw data values\n",
    "xydatar = [[20., 13.0, 310.8, 30.97], [20., 14.5, 308.0, 32.3]]\n",
    "xydatar.append([20., 15.3, 306.0, 31.5])\n",
    "xydatar.append([20.2, 13.0, 310.8, 30.91])\n",
    "xydatar.append([20., 14.5, 308.0, 32.5])\n",
    "xydatar.append([20., 15.3, 306.0, 31.4])\n",
    "xydatar.append([24., 13.0, 310.8, 35.59])\n",
    "xydatar.append([36., 14.5, 308.0, 46.4])\n",
    "print ('Tdbin, Twbin, qdot, Tdbout, ypredicted:')\n",
    "for i in range(0,8):\n",
    "    z1 = w01*xydata[i][0]+w02*xydata[i][1]+w03*xydata[i][2]+b1\n",
    "    sig1 = z1\n",
    "    sigp1 = 1.0\n",
    "    if z1 < 0.0:\n",
    "        sig1 = math.exp(z1) - 1.0\n",
    "        sigp1 = math.exp(z1)\n",
    "    a1 = sig1\n",
    "    z2 = w12*a1+b2\n",
    "    sig2 = z2\n",
    "    sigp2 = 1.0\n",
    "    if z2 < 0.0:\n",
    "        sig2 = math.exp(z2) - 1.0\n",
    "        sigp2 = math.exp(z2)\n",
    "    a2 = sig2\n",
    "    z3 = w23*a2+b3\n",
    "    sig3 = z3\n",
    "    sigp3 = 1.0\n",
    "    if z3 < 0.0:\n",
    "        sig3 = math.exp(z3) - 1.0\n",
    "        sigp3 = math.exp(z3)\n",
    "    a3 = sig3\n",
    "    \n",
    "    print (xydatar[i][0], xydatar[i][1], xydatar[i][2], xydatar[i][3], a3*32.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a4a1d2",
   "metadata": {},
   "source": [
    "TASK 1.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57c6f009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.990099\n",
      "1    0.990099\n",
      "2    0.990099\n",
      "3    1.000000\n",
      "4    0.990099\n",
      "5    1.000000\n",
      "6    1.188119\n",
      "7    1.782178\n",
      "Name: x01, dtype: float64 0    0.896552\n",
      "1    1.000000\n",
      "2    1.055172\n",
      "3    0.896552\n",
      "4    1.000000\n",
      "5    1.055172\n",
      "6    0.896552\n",
      "7    1.000000\n",
      "Name: x02, dtype: float64 0    1.009091\n",
      "1    1.000000\n",
      "2    0.993506\n",
      "3    1.009091\n",
      "4    1.000000\n",
      "5    0.993506\n",
      "6    1.009091\n",
      "7    1.000000\n",
      "Name: x03, dtype: float64 0    0.956452\n",
      "1    0.993796\n",
      "2    0.978365\n",
      "3    0.954292\n",
      "4    0.999969\n",
      "5    0.969106\n",
      "6    1.096571\n",
      "7    1.432055\n",
      "Name: y3, dtype: float64\n",
      "[[0.9900990099009901, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936], [1.0, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [1.0, 1.0551724137931036, 0.9935064935064936], [1.188118811881188, 0.896551724137931, 1.009090909090909], [1.7821782178217822, 1.0, 1.0]]\n",
      "[[0.99009901 0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [0.99009901 1.05517241 0.99350649]\n",
      " [1.         0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [1.         1.05517241 0.99350649]\n",
      " [1.18811881 0.89655172 1.00909091]\n",
      " [1.78217822 1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "'''>>>>> start CodeP2.2F23-updated\n",
    "V.P. Carey ME249, Fall 2023\n",
    "Intro to Neural Network Modeling\n",
    "Keras model for comparison with first principles model'''\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "#raw data in dictionary form x01, x02, x03, y3\n",
    "my_dict = {\n",
    "    'x01' : [20., 20., 20., 20.2, 20., 20.2, 24.0, 36.],\n",
    "    'x02' : [13., 14.5, 15.3, 13., 14.5, 15.3, 13., 14.5],\n",
    "    'x03' : [310.8, 308.0, 306.0, 310.8, 308.0, 306.0, 310.8, 308.0],\n",
    "    'y3' : [30.99, 32.2, 31.7, 30.92, 32.4, 31.4, 35.53, 46.4]\n",
    "}\n",
    "#normalized inputs in array\n",
    "xdata = []\n",
    "xdata = [[20./20.2, 13.0/14.5, 310.8/308.0], [20./20.2, 14.5/14.5, 308.0/308.0]]\n",
    "xdata.append([20./20.2, 15.3/14.5, 306.0/308.0])\n",
    "xdata.append([20.2/20.2, 13.0/14.5, 310.8/308.0])\n",
    "xdata.append([20./20.2, 14.5/14.5, 308.0/308.0])\n",
    "xdata.append([20.2/20.2, 15.3/14.5, 306.0/308.0])\n",
    "xdata.append([24./20.2, 13.0/14.5, 310.8/308.0])\n",
    "xdata.append([36./20.2, 14.5/14.5, 308.0/308.0])\n",
    "#data frame\n",
    "df = pd.DataFrame(my_dict)\n",
    "#devide by the median to normalize\n",
    "df.x01= df.x01/20.2\n",
    "df.x02= df.x02/14.5\n",
    "df.x03= df.x03/308.0\n",
    "#normalize output array\n",
    "df.y3= df.y3/32.401\n",
    "df.head\n",
    "print (df.x01, df.x02, df.x03, df.y3)\n",
    "xarray= np.array(xdata)\n",
    "print (xdata)\n",
    "print (xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b12a521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_one (Dense)            (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "dense_two (Dense)            (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dense_three (Dense)          (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 8\n",
      "Trainable params: 8\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(3, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "#As seen below, we have created three dense layers each with just one neuron.\n",
    "#A dense layer is a layer in neural network that’s fully connected.\n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 3 in this case.\n",
    "#The activation function we have chosen is ReLU, which stands for rectified linear unit.\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=1.2)\n",
    "\n",
    "# define three layer model with one neuron in each layer\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation=K.elu, input_shape=[3], kernel_initializer=initializer, name=\"dense_one\"),\n",
    "    keras.layers.Dense(1, activation=K.elu, kernel_initializer=initializer, name=\"dense_two\"),\n",
    "    keras.layers.Dense(1, activation=K.elu, kernel_initializer=initializer, name=\"dense_three\")\n",
    "])\n",
    "model.summary()\n",
    "#set starting values to those used in first principles model\n",
    "w01n = 1.23\n",
    "w02n = 0.40\n",
    "w03n = 0.70\n",
    "b1n = -0.15\n",
    "w12n = 0.72\n",
    "b2n = -0.12\n",
    "w23n = 0.7\n",
    "b3n = 0.01\n",
    "\n",
    "weights0 = [[ w01n], [w02n], [ w03n]]\n",
    "w0array= np.array(weights0)\n",
    "print(np.shape(w0array))\n",
    "bias0 = [b1n]\n",
    "bias0array= np.array(bias0)\n",
    "L0=[]\n",
    "L0.append(w0array)\n",
    "L0.append(bias0array)\n",
    "model.layers[0].set_weights(L0)\n",
    "\n",
    "weights1 = [[ w12n]]\n",
    "w1array= np.array(weights1)\n",
    "print(np.shape(w1array))\n",
    "bias1 = [b2n]\n",
    "bias1array= np.array(bias1)\n",
    "L1=[]\n",
    "L1.append(w1array)\n",
    "L1.append(bias1array)\n",
    "model.layers[1].set_weights(L1)\n",
    "\n",
    "weights2 = [[ w23n]]\n",
    "w2array= np.array(weights2)\n",
    "print(np.shape(w2array))\n",
    "bias2 = [b3n]\n",
    "bias2array= np.array(bias2)\n",
    "L2=[]\n",
    "L2.append(w2array)\n",
    "L2.append(bias2array)\n",
    "model.layers[2].set_weights(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da23d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation.\n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks.\n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here\n",
    "#is mean absolute error. After the compilation of the model, we’ll use the fit method with 100 epochs.\n",
    "#Running model.fit successive times extends the calculation to addtional epochs.\n",
    "rms = keras.optimizers.RMSprop(0.0035)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a1e6e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0449\n",
      "Epoch 2/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0307\n",
      "Epoch 3/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0237\n",
      "Epoch 4/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0177\n",
      "Epoch 5/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0172\n",
      "Epoch 6/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0147\n",
      "Epoch 7/400\n",
      "1/1 [==============================] - 0s 994us/step - loss: 0.0166\n",
      "Epoch 8/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 9/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0159\n",
      "Epoch 10/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0165\n",
      "Epoch 11/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186\n",
      "Epoch 12/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0144\n",
      "Epoch 13/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0143\n",
      "Epoch 14/400\n",
      "1/1 [==============================] - 0s 995us/step - loss: 0.0144\n",
      "Epoch 15/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 16/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0151\n",
      "Epoch 17/400\n",
      "1/1 [==============================] - 0s 993us/step - loss: 0.0162\n",
      "Epoch 18/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0168\n",
      "Epoch 19/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0313\n",
      "Epoch 20/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0192\n",
      "Epoch 21/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 22/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0159\n",
      "Epoch 23/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0147\n",
      "Epoch 24/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0153\n",
      "Epoch 25/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0156\n",
      "Epoch 26/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0178\n",
      "Epoch 27/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0140\n",
      "Epoch 28/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0139\n",
      "Epoch 29/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0139\n",
      "Epoch 30/400\n",
      "1/1 [==============================] - 0s 993us/step - loss: 0.0165\n",
      "Epoch 31/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0146\n",
      "Epoch 32/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 33/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0166\n",
      "Epoch 34/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0294\n",
      "Epoch 35/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196\n",
      "Epoch 36/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0187\n",
      "Epoch 37/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0154\n",
      "Epoch 38/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0143\n",
      "Epoch 39/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0148\n",
      "Epoch 40/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0157\n",
      "Epoch 41/400\n",
      "1/1 [==============================] - 0s 993us/step - loss: 0.0231\n",
      "Epoch 42/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0146\n",
      "Epoch 43/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0143\n",
      "Epoch 44/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0166\n",
      "Epoch 45/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0207\n",
      "Epoch 46/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0156\n",
      "Epoch 47/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0196\n",
      "Epoch 48/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0138\n",
      "Epoch 49/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0169\n",
      "Epoch 50/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190\n",
      "Epoch 51/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0159\n",
      "Epoch 52/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0135\n",
      "Epoch 53/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0137\n",
      "Epoch 54/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 55/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0146\n",
      "Epoch 56/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0185\n",
      "Epoch 57/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0148\n",
      "Epoch 58/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 59/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0236\n",
      "Epoch 60/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0205\n",
      "Epoch 61/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0176\n",
      "Epoch 62/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 63/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0149\n",
      "Epoch 64/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0175\n",
      "Epoch 65/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0142\n",
      "Epoch 66/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0163\n",
      "Epoch 67/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0208\n",
      "Epoch 68/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0154\n",
      "Epoch 69/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197\n",
      "Epoch 70/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0138\n",
      "Epoch 71/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 72/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190\n",
      "Epoch 73/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0159\n",
      "Epoch 74/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0133\n",
      "Epoch 75/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0134\n",
      "Epoch 76/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0152\n",
      "Epoch 77/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 78/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0185\n",
      "Epoch 79/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0148\n",
      "Epoch 80/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0158\n",
      "Epoch 81/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0239\n",
      "Epoch 82/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0202\n",
      "Epoch 83/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0176\n",
      "Epoch 84/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0145\n",
      "Epoch 85/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0148\n",
      "Epoch 86/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0175\n",
      "Epoch 87/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0142\n",
      "Epoch 88/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 89/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0210\n",
      "Epoch 90/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0152\n",
      "Epoch 91/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0171\n",
      "Epoch 92/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0139\n",
      "Epoch 93/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0167\n",
      "Epoch 94/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199\n",
      "Epoch 95/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 96/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0179\n",
      "Epoch 97/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189\n",
      "Epoch 98/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0157\n",
      "Epoch 99/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0131\n",
      "Epoch 100/400\n",
      "1/1 [==============================] - 0s 994us/step - loss: 0.0151\n",
      "Epoch 101/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 102/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0184\n",
      "Epoch 103/400\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.0147\n",
      "Epoch 104/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0157\n",
      "Epoch 105/400\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.0240\n",
      "Epoch 106/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0201\n",
      "Epoch 107/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0175\n",
      "Epoch 108/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 109/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0148\n",
      "Epoch 110/400\n",
      "1/1 [==============================] - 0s 994us/step - loss: 0.0175\n",
      "Epoch 111/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0142\n",
      "Epoch 112/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0159\n",
      "Epoch 113/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0211\n",
      "Epoch 114/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0153\n",
      "Epoch 115/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "Epoch 116/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 117/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0166\n",
      "Epoch 118/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199\n",
      "Epoch 119/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 120/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0178\n",
      "Epoch 121/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0189\n",
      "Epoch 122/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0130\n",
      "Epoch 123/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0130\n",
      "Epoch 124/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0132\n",
      "Epoch 125/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0159\n",
      "Epoch 126/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0132\n",
      "Epoch 127/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0197\n",
      "Epoch 128/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0130\n",
      "Epoch 129/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0162\n",
      "Epoch 130/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0130\n",
      "Epoch 131/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0137\n",
      "Epoch 132/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0204\n",
      "Epoch 133/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0199\n",
      "Epoch 134/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0187\n",
      "Epoch 135/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0152\n",
      "Epoch 136/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 137/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0185\n",
      "Epoch 138/400\n",
      "1/1 [==============================] - 0s 993us/step - loss: 0.0148\n",
      "Epoch 139/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0153\n",
      "Epoch 140/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0183\n",
      "Epoch 141/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0144\n",
      "Epoch 142/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0167\n",
      "Epoch 143/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0237\n",
      "Epoch 144/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0209\n",
      "Epoch 145/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0173\n",
      "Epoch 146/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 147/400\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.0154\n",
      "Epoch 148/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0172\n",
      "Epoch 149/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0139\n",
      "Epoch 150/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0167\n",
      "Epoch 151/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0205\n",
      "Epoch 152/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0158\n",
      "Epoch 153/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 154/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0136\n",
      "Epoch 155/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0172\n",
      "Epoch 156/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0195\n",
      "Epoch 157/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 158/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0184\n",
      "Epoch 159/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0185\n",
      "Epoch 160/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 161/400\n",
      "1/1 [==============================] - 0s 994us/step - loss: 0.0135\n",
      "Epoch 162/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0184\n",
      "Epoch 163/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0150\n",
      "Epoch 164/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0146\n",
      "Epoch 165/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0183\n",
      "Epoch 166/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0146\n",
      "Epoch 167/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0158\n",
      "Epoch 168/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 169/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0142\n",
      "Epoch 170/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0173\n",
      "Epoch 171/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0231\n",
      "Epoch 172/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0215\n",
      "Epoch 173/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 174/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 175/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0158\n",
      "Epoch 176/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 177/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0136\n",
      "Epoch 178/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0172\n",
      "Epoch 179/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200\n",
      "Epoch 180/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 181/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0184\n",
      "Epoch 182/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0188\n",
      "Epoch 183/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0128\n",
      "Epoch 184/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 185/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0128\n",
      "Epoch 186/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0163\n",
      "Epoch 187/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0128\n",
      "Epoch 188/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0165\n",
      "Epoch 189/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 190/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0132\n",
      "Epoch 191/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0166\n",
      "Epoch 192/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0132\n",
      "Epoch 193/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 194/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 195/400\n",
      "1/1 [==============================] - 0s 995us/step - loss: 0.0171\n",
      "Epoch 196/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 197/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0173\n",
      "Epoch 198/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 199/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0176\n",
      "Epoch 200/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0129\n",
      "Epoch 201/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0178\n",
      "Epoch 202/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 203/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0136\n",
      "Epoch 204/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0286\n",
      "Epoch 205/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0249\n",
      "Epoch 206/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0202\n",
      "Epoch 207/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0158\n",
      "Epoch 208/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0166\n",
      "Epoch 209/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0135\n",
      "Epoch 210/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0172\n",
      "Epoch 211/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 212/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 213/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0184\n",
      "Epoch 214/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0184\n",
      "Epoch 215/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0152\n",
      "Epoch 216/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0136\n",
      "Epoch 217/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0183\n",
      "Epoch 218/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0149\n",
      "Epoch 219/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0147\n",
      "Epoch 220/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 221/400\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.0145\n",
      "Epoch 222/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0158\n",
      "Epoch 223/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0179\n",
      "Epoch 224/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 225/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0173\n",
      "Epoch 226/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0229\n",
      "Epoch 227/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0215\n",
      "Epoch 228/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 229/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 230/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0159\n",
      "Epoch 231/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 232/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0136\n",
      "Epoch 233/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0173\n",
      "Epoch 234/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199\n",
      "Epoch 235/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0130\n",
      "Epoch 236/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0184\n",
      "Epoch 237/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187\n",
      "Epoch 238/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0128\n",
      "Epoch 239/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 240/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 241/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0162\n",
      "Epoch 242/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 243/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0165\n",
      "Epoch 244/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 245/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 246/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0127\n",
      "Epoch 247/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 248/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0127\n",
      "Epoch 249/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0132\n",
      "Epoch 250/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 251/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 252/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0172\n",
      "Epoch 253/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 254/400\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.0175\n",
      "Epoch 255/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 256/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0177\n",
      "Epoch 257/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0129\n",
      "Epoch 258/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 259/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0127\n",
      "Epoch 260/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0136\n",
      "Epoch 261/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0294\n",
      "Epoch 262/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0247\n",
      "Epoch 263/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0206\n",
      "Epoch 264/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0157\n",
      "Epoch 265/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0167\n",
      "Epoch 266/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0136\n",
      "Epoch 267/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0169\n",
      "Epoch 268/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195\n",
      "Epoch 269/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0131\n",
      "Epoch 270/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0181\n",
      "Epoch 271/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0185\n",
      "Epoch 272/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 273/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0134\n",
      "Epoch 274/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0184\n",
      "Epoch 275/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0150\n",
      "Epoch 276/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 277/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0183\n",
      "Epoch 278/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0146\n",
      "Epoch 279/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0157\n",
      "Epoch 280/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 281/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 282/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 283/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0233\n",
      "Epoch 284/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0211\n",
      "Epoch 285/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 286/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 287/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 288/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0169\n",
      "Epoch 289/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0136\n",
      "Epoch 290/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0169\n",
      "Epoch 291/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201\n",
      "Epoch 292/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0161\n",
      "Epoch 293/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0165\n",
      "Epoch 294/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0133\n",
      "Epoch 295/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0175\n",
      "Epoch 296/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0192\n",
      "Epoch 297/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 298/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187\n",
      "Epoch 299/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0183\n",
      "Epoch 300/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0150\n",
      "Epoch 301/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0138\n",
      "Epoch 302/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 303/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0147\n",
      "Epoch 304/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0150\n",
      "Epoch 305/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 306/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0143\n",
      "Epoch 307/400\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.0161\n",
      "Epoch 308/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0177\n",
      "Epoch 309/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0139\n",
      "Epoch 310/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0176\n",
      "Epoch 311/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0226\n",
      "Epoch 312/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0217\n",
      "Epoch 313/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0167\n",
      "Epoch 314/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0137\n",
      "Epoch 315/400\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.0162\n",
      "Epoch 316/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 317/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0134\n",
      "Epoch 318/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0175\n",
      "Epoch 319/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0197\n",
      "Epoch 320/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0129\n",
      "Epoch 321/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0134\n",
      "Epoch 322/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0178\n",
      "Epoch 323/400\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0198\n",
      "Epoch 324/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0128\n",
      "Epoch 325/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189\n",
      "Epoch 326/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186\n",
      "Epoch 327/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 328/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0158Restoring model weights from the end of the best epoch.\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0158\n",
      "Epoch 00328: early stopping\n",
      "best epoch =  248\n",
      "smallest loss = 0.01270446926355362\n"
     ]
    }
   ],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training.\n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again.\n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs\n",
    "#I found acceptable prediction accuracy.\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs.\n",
    "#During model training, if all the batches of data are seen by the model once,\n",
    "#we say that one epoch has been completed.\n",
    "# Add an early stopping callback\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    patience = 80,\n",
    "    restore_best_weights = True,\n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss',\n",
    "mode='min', verbose=1, save_best_only=True)\n",
    "historyData = model.fit(xarray,df.y3,epochs=400,callbacks=[es])\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15cbc6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2057927 ]\n",
      " [0.38605163]\n",
      " [0.7068726 ]]\n",
      "w01 =  1.2057927 w02 =  0.38605163 w03 =  0.7068726\n",
      "[-0.14522088]\n",
      "b1 =  [-0.14522088]\n",
      "[[0.70472664]]\n",
      "w12 =  0.70472664\n",
      "[-0.11338988]\n",
      "b2 =  [-0.11338988]\n",
      "[[0.6820863]]\n",
      "w23 =  0.6820863\n",
      "[0.01850359]\n",
      "b3 =  [0.01850359]\n",
      "x01/20.2, x02/14.5, x03/308.0, y3/32.4, a3:\n",
      "0.9900990099009901 0.896551724137931 1.009090909090909 0.9564519613592172 [[0.95446724]]\n",
      "0.9900990099009901 1.0 1.0 0.9937964877627233 [[0.9705753]]\n",
      "0.9900990099009901 1.0551724137931036 0.9935064935064936 0.9783648652819357 [[0.97860694]]\n",
      "1.0 0.896551724137931 1.009090909090909 0.954291534211907 [[0.9602059]]\n",
      "0.9900990099009901 1.0 1.0 0.9999691367550383 [[0.9705753]]\n",
      "1.0 1.0551724137931036 0.9935064935064936 0.9691058917934631 [[0.9843456]]\n",
      "1.188118811881188 0.896551724137931 1.009090909090909 1.096571093484769 [[1.0692406]]\n",
      "1.7821782178217822 1.0 1.0 1.4320545662170918 [[1.4296689]]\n",
      " \n",
      "x01, x02, x03, y3, a3*32.4:\n",
      "20.0 13.0 310.8 30.989043548038634 [[30.92474]]\n",
      "20.0 14.5 308.0 32.19900620351223 [[31.44664]]\n",
      "20.0 15.3 306.0 31.699021635134713 [[31.706867]]\n",
      "20.2 13.0 310.8 30.919045708465788 [[31.110674]]\n",
      "20.0 14.5 308.0 32.39900003086324 [[31.44664]]\n",
      "20.2 15.3 306.0 31.3990308941082 [[31.8928]]\n",
      "23.999999999999996 13.0 310.8 35.52890342890652 [[34.643394]]\n",
      "36.0 14.5 308.0 46.398567945433776 [[46.321274]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#For results of training network:\n",
    "#keras.layer.get_weights() function retrieves weight values\n",
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "w01 = first_layer_weights[0][0]\n",
    "w02 = first_layer_weights[1][0]\n",
    "w03 = first_layer_weights[2][0]\n",
    "first_layer_bias = model.layers[0].get_weights()[1]\n",
    "b1 = first_layer_bias\n",
    "second_layer_weights = model.layers[1].get_weights()[0]\n",
    "w12 = second_layer_weights[0][0]\n",
    "second_layer_bias = model.layers[1].get_weights()[1]\n",
    "b2 = second_layer_bias\n",
    "third_layer_weights = model.layers[2].get_weights()[0]\n",
    "w23 = third_layer_weights[0][0]\n",
    "third_layer_bias = model.layers[2].get_weights()[1]\n",
    "b3 = third_layer_bias\n",
    "#print weights and biases\n",
    "print (first_layer_weights)\n",
    "print ('w01 = ', w01, 'w02 = ', w02, 'w03 = ', w03)\n",
    "print (first_layer_bias)\n",
    "print ('b1 = ', b1)\n",
    "print (second_layer_weights)\n",
    "print ('w12 = ', w12)\n",
    "print (second_layer_bias)\n",
    "print ('b2 = ', b2)\n",
    "print (third_layer_weights)\n",
    "print ('w23 = ', w23)\n",
    "print (third_layer_bias)\n",
    "print ('b3 = ', b3)\n",
    "#use model.predict() function to print model predictions for data conditions\n",
    "xarray= np.array(xdata)\n",
    "print ('x01/20.2, x02/14.5, x03/308.0, y3/32.4, a3:')\n",
    "test = []\n",
    "for i in range(0,8):\n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0], xarray[i][1], xarray[i][2], df.y3[i], a3)\n",
    "print(' ')\n",
    "print ('x01, x02, x03, y3, a3*32.4:')\n",
    "for i in range(0,8):\n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0]*20.2, xarray[i][1]*14.5, xarray[i][2]*308.0, df.y3[i]*32.4, a3*32.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df923b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
