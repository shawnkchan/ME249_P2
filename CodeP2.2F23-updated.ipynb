{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.990099\n",
      "1    0.990099\n",
      "2    0.990099\n",
      "3    1.000000\n",
      "4    0.990099\n",
      "5    1.000000\n",
      "6    1.188119\n",
      "7    1.782178\n",
      "Name: x01, dtype: float64 0    0.896552\n",
      "1    1.000000\n",
      "2    1.055172\n",
      "3    0.896552\n",
      "4    1.000000\n",
      "5    1.055172\n",
      "6    0.896552\n",
      "7    1.000000\n",
      "Name: x02, dtype: float64 0    1.009091\n",
      "1    1.000000\n",
      "2    0.993506\n",
      "3    1.009091\n",
      "4    1.000000\n",
      "5    0.993506\n",
      "6    1.009091\n",
      "7    1.000000\n",
      "Name: x03, dtype: float64 0    0.956452\n",
      "1    0.993796\n",
      "2    0.978365\n",
      "3    0.954292\n",
      "4    0.999969\n",
      "5    0.969106\n",
      "6    1.096571\n",
      "7    1.432055\n",
      "Name: y3, dtype: float64\n",
      "[[0.9900990099009901, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936], [1.0, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [1.0, 1.0551724137931036, 0.9935064935064936], [1.188118811881188, 0.896551724137931, 1.009090909090909], [1.7821782178217822, 1.0, 1.0]]\n",
      "[[0.99009901 0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [0.99009901 1.05517241 0.99350649]\n",
      " [1.         0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [1.         1.05517241 0.99350649]\n",
      " [1.18811881 0.89655172 1.00909091]\n",
      " [1.78217822 1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "'''>>>>> start CodeP2.2F23-updated\n",
    "    V.P. Carey ME249, Fall 2023\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Keras model for comparison with first principles model'''\n",
    "\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#raw data in dictionary form x01, x02, x03, y3\n",
    "my_dict = { \n",
    "    'x01' : [20., 20., 20., 20.2, 20., 20.2, 24.0, 36.],\n",
    "    'x02' : [13., 14.5, 15.3, 13., 14.5, 15.3, 13., 14.5],\n",
    "    'x03' : [310.8, 308.0, 306.0, 310.8, 308.0, 306.0, 310.8, 308.0],\n",
    "    'y3' : [30.99, 32.2, 31.7, 30.92, 32.4, 31.4, 35.53, 46.4]\n",
    "}\n",
    "#normalized inputs in array\n",
    "xdata = []\n",
    "xdata = [[20./20.2, 13.0/14.5, 310.8/308.0], [20./20.2, 14.5/14.5, 308.0/308.0]] \n",
    "xdata.append([20./20.2, 15.3/14.5, 306.0/308.0])\n",
    "xdata.append([20.2/20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([20./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "xdata.append([20.2/20.2, 15.3/14.5, 306.0/308.0]) \n",
    "xdata.append([24./20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([36./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "\n",
    "#data frame\n",
    "df = pd.DataFrame(my_dict)\n",
    "#devide by the median to normalize \n",
    "df.x01= df.x01/20.2\n",
    "df.x02= df.x02/14.5\n",
    "df.x03= df.x03/308.0\n",
    "#normalize output array\n",
    "df.y3= df.y3/32.401\n",
    "df.head\n",
    "print (df.x01, df.x02, df.x03, df.y3)\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "print (xdata)\n",
    "print (xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_one (Dense)            (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "dense_two (Dense)            (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dense_three (Dense)          (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 8\n",
      "Trainable params: 8\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(3, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 02:04:37.309916: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-23 02:04:37.313382: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "#As seen below, we have created three dense layers each with just one neuron. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 3 in this case. \n",
    "#The activation function we have chosen is ReLU, which stands for rectified linear unit.\n",
    "\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=1.2)\n",
    "\n",
    "# define three layer model with one neuron in each layer\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation=K.elu, input_shape=[3],  kernel_initializer=initializer, name=\"dense_one\"),\n",
    "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_two\"),\n",
    "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_three\")\n",
    "  ])\n",
    "model.summary()\n",
    "\n",
    "#set starting values to those used in first principles model\n",
    "w01n =  1.23 \n",
    "w02n =  0.40 \n",
    "w03n =  0.70\n",
    "b1n =  -0.15\n",
    "w12n =  0.72\n",
    "b2n =  -0.12\n",
    "w23n =  0.7\n",
    "b3n =  0.01\n",
    "\n",
    "weights0 =  [[ w01n], [w02n], [ w03n]]\n",
    "w0array= np.array(weights0)\n",
    "print(np.shape(w0array))\n",
    "bias0 = [b1n]\n",
    "bias0array= np.array(bias0)\n",
    "L0=[]\n",
    "L0.append(w0array)\n",
    "L0.append(bias0array)\n",
    "model.layers[0].set_weights(L0) \n",
    "\n",
    "weights1 =  [[ w12n]]\n",
    "w1array= np.array(weights1)\n",
    "print(np.shape(w1array))\n",
    "bias1 = [b2n]\n",
    "bias1array= np.array(bias1)\n",
    "L1=[]\n",
    "L1.append(w1array)\n",
    "L1.append(bias1array)\n",
    "model.layers[1].set_weights(L1)\n",
    "\n",
    "weights2 =  [[ w23n]]\n",
    "w2array= np.array(weights2)\n",
    "print(np.shape(w2array))\n",
    "bias2 = [b3n]\n",
    "bias2array= np.array(bias2)\n",
    "L2=[]\n",
    "L2.append(w2array)\n",
    "L2.append(bias2array)\n",
    "model.layers[2].set_weights(L2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean absolute error. After the compilation of the model, we’ll use the fit method with 100 epochs.\n",
    "\n",
    "#Running model.fit successive times extends the calculation to addtional epochs.\n",
    "\n",
    "rms = keras.optimizers.RMSprop(0.0035)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms, metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.0132 - mae: 0.0132\n",
      "Epoch 2/800\n",
      "8/8 [==============================] - 0s 175us/step - loss: 0.0675 - mae: 0.0675\n",
      "Epoch 3/800\n",
      "8/8 [==============================] - 0s 151us/step - loss: 0.0127 - mae: 0.0127\n",
      "Epoch 4/800\n",
      "8/8 [==============================] - 0s 146us/step - loss: 0.0244 - mae: 0.0244\n",
      "Epoch 5/800\n",
      "8/8 [==============================] - 0s 127us/step - loss: 0.0284 - mae: 0.0284\n",
      "Epoch 6/800\n",
      "8/8 [==============================] - 0s 142us/step - loss: 0.0185 - mae: 0.0185\n",
      "Epoch 7/800\n",
      "8/8 [==============================] - 0s 164us/step - loss: 0.0182 - mae: 0.0182\n",
      "Epoch 8/800\n",
      "8/8 [==============================] - 0s 147us/step - loss: 0.0151 - mae: 0.0151\n",
      "Epoch 9/800\n",
      "8/8 [==============================] - 0s 174us/step - loss: 0.0138 - mae: 0.0138\n",
      "Epoch 10/800\n",
      "8/8 [==============================] - 0s 218us/step - loss: 0.0181 - mae: 0.0181\n",
      "Epoch 11/800\n",
      "8/8 [==============================] - 0s 168us/step - loss: 0.0147 - mae: 0.0147\n",
      "Epoch 12/800\n",
      "8/8 [==============================] - 0s 154us/step - loss: 0.0149 - mae: 0.0149\n",
      "Epoch 13/800\n",
      "8/8 [==============================] - 0s 190us/step - loss: 0.0180 - mae: 0.0180\n",
      "Epoch 14/800\n",
      "8/8 [==============================] - 0s 490us/step - loss: 0.0143 - mae: 0.0143\n",
      "Epoch 15/800\n",
      "8/8 [==============================] - 0s 166us/step - loss: 0.0160 - mae: 0.0160\n",
      "Epoch 16/800\n",
      "8/8 [==============================] - 0s 168us/step - loss: 0.0177 - mae: 0.0177\n",
      "Epoch 17/800\n",
      "8/8 [==============================] - 0s 139us/step - loss: 0.0139 - mae: 0.0139\n",
      "Epoch 18/800\n",
      "8/8 [==============================] - 0s 161us/step - loss: 0.0175 - mae: 0.0175\n",
      "Epoch 19/800\n",
      "8/8 [==============================] - 0s 161us/step - loss: 0.0224 - mae: 0.0224\n",
      "Epoch 20/800\n",
      "8/8 [==============================] - 0s 190us/step - loss: 0.0161 - mae: 0.0161\n",
      "Epoch 21/800\n",
      "8/8 [==============================] - 0s 142us/step - loss: 0.0170 - mae: 0.0170\n",
      "Epoch 22/800\n",
      "8/8 [==============================] - 0s 149us/step - loss: 0.0136 - mae: 0.0136\n",
      "Epoch 23/800\n",
      "8/8 [==============================] - 0s 170us/step - loss: 0.0174 - mae: 0.0174\n",
      "Epoch 24/800\n",
      "8/8 [==============================] - 0s 149us/step - loss: 0.0202 - mae: 0.0202\n",
      "Epoch 25/800\n",
      "8/8 [==============================] - 0s 159us/step - loss: 0.0164 - mae: 0.0164\n",
      "Epoch 26/800\n",
      "8/8 [==============================] - 0s 162us/step - loss: 0.0164 - mae: 0.0164\n",
      "Epoch 27/800\n",
      "8/8 [==============================] - 0s 162us/step - loss: 0.0132 - mae: 0.0132\n",
      "Epoch 28/800\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0179 - mae: 0.0179\n",
      "Epoch 29/800\n",
      "8/8 [==============================] - 0s 150us/step - loss: 0.0191 - mae: 0.0191\n",
      "Epoch 30/800\n",
      "8/8 [==============================] - 0s 157us/step - loss: 0.0128 - mae: 0.0128\n",
      "Epoch 31/800\n",
      "8/8 [==============================] - 0s 173us/step - loss: 0.0132 - mae: 0.0132\n",
      "Epoch 32/800\n",
      "8/8 [==============================] - 0s 165us/step - loss: 0.0182 - mae: 0.0182\n",
      "Epoch 33/800\n",
      "8/8 [==============================] - 0s 161us/step - loss: 0.0194 - mae: 0.0194\n",
      "Epoch 34/800\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0128 - mae: 0.0128\n",
      "Epoch 35/800\n",
      "8/8 [==============================] - 0s 222us/step - loss: 0.0132 - mae: 0.0132\n",
      "Epoch 36/800\n",
      "8/8 [==============================] - 0s 164us/step - loss: 0.0185 - mae: 0.0185\n",
      "Epoch 37/800\n",
      "8/8 [==============================] - 0s 137us/step - loss: 0.0195 - mae: 0.0195\n",
      "Epoch 38/800\n",
      "8/8 [==============================] - 0s 187us/step - loss: 0.0127 - mae: 0.0127\n",
      "Epoch 39/800\n",
      "8/8 [==============================] - 0s 168us/step - loss: 0.0132 - mae: 0.0132\n",
      "Epoch 40/800\n",
      "8/8 [==============================] - 0s 186us/step - loss: 0.0187 - mae: 0.0187\n",
      "Epoch 41/800\n",
      "8/8 [==============================] - 0s 164us/step - loss: 0.0195 - mae: 0.0195\n",
      "Epoch 42/800\n",
      "8/8 [==============================] - 0s 145us/step - loss: 0.0127 - mae: 0.0127\n",
      "Epoch 43/800\n",
      "8/8 [==============================] - 0s 141us/step - loss: 0.0132 - mae: 0.0132\n",
      "Epoch 44/800\n",
      "8/8 [==============================] - 0s 132us/step - loss: 0.0188 - mae: 0.0188\n",
      "Epoch 45/800\n",
      "8/8 [==============================] - 0s 169us/step - loss: 0.0196 - mae: 0.0196\n",
      "Epoch 46/800\n",
      "8/8 [==============================] - 0s 167us/step - loss: 0.0126 - mae: 0.0126\n",
      "Epoch 47/800\n",
      "8/8 [==============================] - 0s 193us/step - loss: 0.0130 - mae: 0.0130\n",
      "Epoch 48/800\n",
      "8/8 [==============================] - 0s 172us/step - loss: 0.0160 - mae: 0.0160\n",
      "Epoch 49/800\n",
      "8/8 [==============================] - 0s 135us/step - loss: 0.0130 - mae: 0.0130\n",
      "Epoch 50/800\n",
      "8/8 [==============================] - 0s 213us/step - loss: 0.0163 - mae: 0.0163\n",
      "Epoch 51/800\n",
      "8/8 [==============================] - 0s 141us/step - loss: 0.0130 - mae: 0.0130\n",
      "Epoch 52/800\n",
      "8/8 [==============================] - 0s 171us/step - loss: 0.0166 - mae: 0.0166\n",
      "Epoch 53/800\n",
      "8/8 [==============================] - 0s 373us/step - loss: 0.0130 - mae: 0.0130\n",
      "Epoch 54/800\n",
      "8/8 [==============================] - 0s 157us/step - loss: 0.0168 - mae: 0.0168\n",
      "Epoch 55/800\n",
      "8/8 [==============================] - 0s 156us/step - loss: 0.0129 - mae: 0.0129\n",
      "Epoch 56/800\n",
      "8/8 [==============================] - 0s 293us/step - loss: 0.0171 - mae: 0.0171\n",
      "Epoch 57/800\n",
      "8/8 [==============================] - 0s 156us/step - loss: 0.0128 - mae: 0.0128\n",
      "Epoch 58/800\n",
      "8/8 [==============================] - 0s 167us/step - loss: 0.0174 - mae: 0.0174\n",
      "Epoch 59/800\n",
      "8/8 [==============================] - 0s 171us/step - loss: 0.0128 - mae: 0.0128\n",
      "Epoch 60/800\n",
      "8/8 [==============================] - 0s 162us/step - loss: 0.0176 - mae: 0.0176\n",
      "Epoch 61/800\n",
      "8/8 [==============================] - 0s 148us/step - loss: 0.0127 - mae: 0.0127\n",
      "Epoch 62/800\n",
      "8/8 [==============================] - 0s 161us/step - loss: 0.0135 - mae: 0.0135\n",
      "Epoch 63/800\n",
      "8/8 [==============================] - 0s 171us/step - loss: 0.0276 - mae: 0.0276\n",
      "Epoch 64/800\n",
      "8/8 [==============================] - 0s 145us/step - loss: 0.0250 - mae: 0.0250\n",
      "Epoch 65/800\n",
      "8/8 [==============================] - 0s 148us/step - loss: 0.0198 - mae: 0.0198\n",
      "Epoch 66/800\n",
      "8/8 [==============================] - 0s 201us/step - loss: 0.0132 - mae: 0.0132\n",
      "Epoch 67/800\n",
      "8/8 [==============================] - 0s 161us/step - loss: 0.0179 - mae: 0.0179\n",
      "Epoch 68/800\n",
      "8/8 [==============================] - 0s 160us/step - loss: 0.0187 - mae: 0.0187\n",
      "Epoch 69/800\n",
      "8/8 [==============================] - 0s 162us/step - loss: 0.0127 - mae: 0.0127\n",
      "Epoch 70/800\n",
      "8/8 [==============================] - 0s 193us/step - loss: 0.0191 - mae: 0.0191\n",
      "Epoch 71/800\n",
      "8/8 [==============================] - 0s 164us/step - loss: 0.0181 - mae: 0.0181\n",
      "Epoch 72/800\n",
      "8/8 [==============================] - 0s 169us/step - loss: 0.0149 - mae: 0.0149\n",
      "Epoch 73/800\n",
      "8/8 [==============================] - 0s 153us/step - loss: 0.0141 - mae: 0.0141\n",
      "Epoch 74/800\n",
      "8/8 [==============================] - 0s 222us/step - loss: 0.0180 - mae: 0.0180\n",
      "Epoch 75/800\n",
      "8/8 [==============================] - 0s 182us/step - loss: 0.0146 - mae: 0.0146\n",
      "Epoch 76/800\n",
      "8/8 [==============================] - 0s 150us/step - loss: 0.0152 - mae: 0.0152\n",
      "Epoch 77/800\n",
      "8/8 [==============================] - 0s 149us/step - loss: 0.0179 - mae: 0.0179\n",
      "Epoch 78/800\n",
      "8/8 [==============================] - 0s 152us/step - loss: 0.0142 - mae: 0.0142\n",
      "Epoch 79/800\n",
      "8/8 [==============================] - 0s 153us/step - loss: 0.0163 - mae: 0.0163\n",
      "Epoch 80/800\n",
      "8/8 [==============================] - 0s 152us/step - loss: 0.0176 - mae: 0.0176\n",
      "Epoch 81/800\n",
      "8/8 [==============================] - 0s 188us/step - loss: 0.0138 - mae: 0.0138\n",
      "Epoch 82/800\n",
      "8/8 [==============================] - 0s 168us/step - loss: 0.0180 - mae: 0.0180\n",
      "Epoch 83/800\n",
      "8/8 [==============================] - 0s 218us/step - loss: 0.0220 - mae: 0.0220\n",
      "Epoch 84/800\n",
      "8/8 [==============================] - 0s 195us/step - loss: 0.0164 - mae: 0.0164\n",
      "Epoch 85/800\n",
      "8/8 [==============================] - 0s 165us/step - loss: 0.0168 - mae: 0.0168\n",
      "Epoch 86/800\n",
      "8/8 [==============================] - 0s 161us/step - loss: 0.0134 - mae: 0.0134\n",
      "Epoch 87/800\n",
      "8/8 [==============================] - 0s 169us/step - loss: 0.0179 - mae: 0.0179\n",
      "Epoch 88/800\n",
      "8/8 [==============================] - 0s 198us/step - loss: 0.0198 - mae: 0.0198\n",
      "Epoch 89/800\n",
      "8/8 [==============================] - 0s 192us/step - loss: 0.0168 - mae: 0.0168\n",
      "Epoch 90/800\n",
      "8/8 [==============================] - 0s 171us/step - loss: 0.0188 - mae: 0.0188\n",
      "Epoch 91/800\n",
      "8/8 [==============================] - 0s 182us/step - loss: 0.0129 - mae: 0.0129\n",
      "Epoch 92/800\n",
      "8/8 [==============================] - 0s 223us/step - loss: 0.0180 - mae: 0.0180\n",
      "Epoch 93/800\n",
      "8/8 [==============================] - 0s 177us/step - loss: 0.0182 - mae: 0.0182\n",
      "Epoch 94/800\n",
      "8/8 [==============================] - 0s 205us/step - loss: 0.0151 - mae: 0.0151\n",
      "Epoch 95/800\n",
      "8/8 [==============================] - 0s 272us/step - loss: 0.0135 - mae: 0.0135\n",
      "Epoch 96/800\n",
      "8/8 [==============================] - 0s 239us/step - loss: 0.0181 - mae: 0.0181\n",
      "Epoch 97/800\n",
      "8/8 [==============================] - 0s 299us/step - loss: 0.0148 - mae: 0.0148\n",
      "Epoch 98/800\n",
      "8/8 [==============================] - 0s 315us/step - loss: 0.0146 - mae: 0.0146\n",
      "Epoch 99/800\n",
      "8/8 [==============================] - 0s 232us/step - loss: 0.0180 - mae: 0.0180\n",
      "Epoch 100/800\n",
      "8/8 [==============================] - 0s 286us/step - loss: 0.0144 - mae: 0.0144\n",
      "Epoch 101/800\n",
      "8/8 [==============================] - 0s 312us/step - loss: 0.0158 - mae: 0.0158\n",
      "Epoch 102/800\n",
      "8/8 [==============================] - 0s 226us/step - loss: 0.0178 - mae: 0.0178\n",
      "Epoch 103/800\n",
      "8/8 [==============================] - 0s 188us/step - loss: 0.0140 - mae: 0.0140\n",
      "Epoch 104/800\n",
      "8/8 [==============================] - 0s 210us/step - loss: 0.0170 - mae: 0.0170\n",
      "Epoch 105/800\n",
      "8/8 [==============================] - 0s 218us/step - loss: 0.0227 - mae: 0.0227\n",
      "Epoch 106/800\n",
      "8/8 [==============================] - 0s 168us/step - loss: 0.0213 - mae: 0.0213\n",
      "Epoch 107/800\n",
      "8/8 [==============================] - 0s 176us/step - loss: 0.0168 - mae: 0.0168\n",
      "Epoch 108/800\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0138 - mae: 0.0138\n",
      "Epoch 109/800\n",
      "8/8 [==============================] - 0s 286us/step - loss: 0.0159 - mae: 0.0159\n",
      "Epoch 110/800\n",
      "8/8 [==============================] - 0s 183us/step - loss: 0.0168 - mae: 0.0168\n",
      "Epoch 111/800\n",
      "8/8 [==============================] - 0s 173us/step - loss: 0.0135 - mae: 0.0135\n",
      "Epoch 112/800\n",
      "8/8 [==============================] - 0s 158us/step - loss: 0.0171 - mae: 0.0171\n",
      "Epoch 113/800\n",
      "8/8 [==============================] - 0s 143us/step - loss: 0.0198 - mae: 0.0198\n",
      "Epoch 114/800\n",
      "8/8 [==============================] - 0s 159us/step - loss: 0.0130 - mae: 0.0130\n",
      "Epoch 115/800\n",
      "8/8 [==============================] - 0s 156us/step - loss: 0.0183 - mae: 0.0183\n",
      "Epoch 116/800\n",
      "8/8 [==============================] - 0s 183us/step - loss: 0.0187 - mae: 0.0187\n",
      "Epoch 117/800\n",
      "8/8 [==============================] - 0s 177us/step - loss: 0.0129 - mae: 0.0129\n",
      "Epoch 118/800\n",
      "8/8 [==============================] - 0s 232us/step - loss: 0.0130 - mae: 0.0130\n",
      "Epoch 119/800\n",
      "8/8 [==============================] - 0s 206us/step - loss: 0.0186 - mae: 0.0186\n",
      "Epoch 120/800\n",
      "8/8 [==============================] - 0s 159us/step - loss: 0.0189 - mae: 0.0189\n",
      "Epoch 121/800\n",
      "8/8 [==============================] - 0s 170us/step - loss: 0.0128 - mae: 0.0128\n",
      "Epoch 122/800\n",
      "8/8 [==============================] - 0s 173us/step - loss: 0.0130 - mae: 0.0130\n",
      "Epoch 123/800\n",
      "8/8 [==============================] - 0s 182us/step - loss: 0.0189 - mae: 0.0189\n",
      "Epoch 124/800\n",
      "8/8 [==============================] - 0s 174us/step - loss: 0.0191 - mae: 0.0191\n",
      "Epoch 125/800\n",
      "8/8 [==============================] - 0s 160us/step - loss: 0.0128 - mae: 0.0128\n",
      "Epoch 126/800\n",
      "8/8 [==============================] - 0s 147us/step - loss: 0.0130 - mae: 0.0130\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00126: early stopping\n",
      "best epoch =  46\n",
      "smallest loss = 0.012647219002246857\n"
     ]
    }
   ],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 80, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)\n",
    "\n",
    "historyData = model.fit(xarray,df.y3,epochs=800,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w01 =  1.2062007 w02 =  0.3837218 w03 =  0.7075059\n",
      "b1 =  [-0.1448993]\n",
      "w12 =  0.70468175\n",
      "b2 =  [-0.11271143]\n",
      "w23 =  0.6818131\n",
      "b3 =  [0.01955017]\n",
      "x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:\n",
      "0.9900990099009901 0.896551724137931 1.009090909090909 0.9564519613592172 [[0.95518905]]\n",
      "0.9900990099009901 1.0 1.0 0.9937964877627233 [[0.97117084]]\n",
      "0.9900990099009901 1.0551724137931036 0.9935064935064936 0.9783648652819357 [[0.97913533]]\n",
      "1.0 0.896551724137931 1.009090909090909 0.954291534211907 [[0.960927]]\n",
      "0.9900990099009901 1.0 1.0 0.9999691367550383 [[0.97117084]]\n",
      "1.0 1.0551724137931036 0.9935064935064936 0.9691058917934631 [[0.98487335]]\n",
      "1.188118811881188 0.896551724137931 1.009090909090909 1.096571093484769 [[1.0699481]]\n",
      "1.7821782178217822 1.0 1.0 1.4320545662170918 [[1.4302068]]\n",
      "  \n",
      "x01,  x02,   x03,  y3,  a3*32.4:\n",
      "20.0 13.0 310.8 30.989043548038634 [[30.948126]]\n",
      "20.0 14.5 308.0 32.19900620351223 [[31.465937]]\n",
      "20.0 15.3 306.0 31.699021635134713 [[31.723986]]\n",
      "20.2 13.0 310.8 30.919045708465788 [[31.134037]]\n",
      "20.0 14.5 308.0 32.39900003086324 [[31.465937]]\n",
      "20.2 15.3 306.0 31.3990308941082 [[31.909899]]\n",
      "23.999999999999996 13.0 310.8 35.52890342890652 [[34.66632]]\n",
      "36.0 14.5 308.0 46.398567945433776 [[46.338703]]\n",
      "MAE = 0.012990795075893402\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#For results of training network:\n",
    "\n",
    "#keras.layer.get_weights() function retrieves weight values\n",
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "w01 = first_layer_weights[0][0]\n",
    "w02 = first_layer_weights[1][0]\n",
    "w03 = first_layer_weights[2][0]\n",
    "first_layer_bias  = model.layers[0].get_weights()[1]\n",
    "b1 = first_layer_bias\n",
    "second_layer_weights = model.layers[1].get_weights()[0]\n",
    "w12 = second_layer_weights[0][0]\n",
    "second_layer_bias  = model.layers[1].get_weights()[1]\n",
    "b2 = second_layer_bias\n",
    "third_layer_weights = model.layers[2].get_weights()[0]\n",
    "w23 = third_layer_weights[0][0]\n",
    "third_layer_bias  = model.layers[2].get_weights()[1]\n",
    "b3 = third_layer_bias\n",
    "\n",
    "#print weights and biases\n",
    "# print (first_layer_weights)\n",
    "print ('w01 = ', w01, 'w02 = ', w02, 'w03 = ', w03)\n",
    "# print (first_layer_bias)\n",
    "print ('b1 = ', b1)\n",
    "# print (second_layer_weights)\n",
    "print ('w12 = ', w12)\n",
    "# print (second_layer_bias)\n",
    "print ('b2 = ', b2)\n",
    "# print (third_layer_weights)\n",
    "print ('w23 = ', w23)\n",
    "# print (third_layer_bias)\n",
    "print ('b3 = ', b3)\n",
    "\n",
    "#use model.predict() function to print model predictions for data conditions\n",
    "xarray= np.array(xdata)\n",
    "print ('x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:')\n",
    "test = []\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0], xarray[i][1], xarray[i][2], df.y3[i], a3)\n",
    "print('  ')\n",
    "print ('x01,  x02,   x03,  y3,  a3*32.4:')\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0]*20.2, xarray[i][1]*14.5, xarray[i][2]*308.0, df.y3[i]*32.4, a3*32.4)\n",
    "\n",
    "mae = historyData.history['mae'][-1]\n",
    "print(f\"MAE = {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
